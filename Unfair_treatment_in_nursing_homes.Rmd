---
title: 'The effect of nursing home status on hospitalisation: A toy example'
author: "Hristo Inouzhe"
date: "`r Sys.Date()`"
output: html_document
references:
- id: biasNursing
  title: 'A study on group fairness in healthcare outcomes for nursing home residents during the COVID-19 pandemic in the Basque Country'
  URL: 'https://arxiv.org/abs'
  type: article-journal
  issued:
    year: 2023
- id: sensAnalysis
  title: 'Impact of nursing home status on healthcare outcomes: the Basque Country's case during the COVID-19 pandemic'
  author:
  - family: Veitch
    given: Victor
  - family: Zaveri
    given: Anisha
  URL: 'https://arxiv.org/abs/2003.01747'
  type: article-journal
  issued:
    year: 2020
- id: MatchIt
  title: 'MatchIt: Nonparametric preprocessing for parametric causal inference'
  author:
  - family: Ho
    given: D.
  - family: Imai
    given: K.
  - family: King
    given: G.
  - family: Stuart
    given: E.
  URL: 'https://www.jstatsoft.org/article/view/v042i08'
  type: article-journal
  issued:
    year: 2011
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE)
```

# Introduction

This is a short tutorial based on the article @biasNursing. Our main goal is for the reader to be able to apply the methods to their own data. Hence, we provide all the necessary tools with meaningful examples.

We start by requiring libraries and sourcing functions:
```{r libraries, message = FALSE}
require(dplyr)
require(transport)
require(ggplot2)
require(reshape2)
require(cubature)
require(ks)
require(MatchIt)
source("Functions/totalVariationAnalysis.R")
source("Functions/weightedMeansDifference.R")
source("Functions/wasserDistance.R")
```

Since data protection restrict us from using the data in @biasNursing we use a synthetic dataset generated artificially to imitate the data, where the covariates of interest are $\mathbf{X}=\{age, Charlson\_index, time\_to\_positive, dementia\}$, the treatment variable $Z$ is $nursing\_home$ and the output variable $Y$ is hospitalisation. 

```{r dataNursing}
dataNursingHomes <- read.csv("Data/nursing_home_synthetic.csv")
head(dataNursingHomes)
```

# Matching with Euclidean distance

To perform a matching based on the (squared) Euclidean distance we compute it, where we also make a normalisation to equiparate the different variables 
```{r distEuclidean}
dist.Euclidean <- as.matrix(dist(cbind((dataNursingHomes$age - 60) / (105 - 60),
                                            dataNursingHomes$Charlson_index / 20,
                                            dataNursingHomes$time_to_positive / 121,
                                            dataNursingHomes$dementia
))^2)
```
The matching procedure is based on solving approximately a 1-to-1 optimal matching problem, where the approximation is based on nearest neighbours.  More info can be found in @MatchIt.
```{r euclideanMatching}
# We create the model for matching, where we match nursing home residents and
# non-residents with respect to the four confounders of interest
formula_obj <- as.formula("nursing_home ~ age + `Charlson_index` + `time_to_positive` + `dementia`")

# We use 1-to-1 optimal matching by the nearest-neighbour approximation in the MatchIt package 
MatchIt.euc.nearest <- MatchIt::matchit(formula_obj,
                                          data = dataNursingHomes,
                                          method = "nearest",
                                          distance = dist.Euclidean)
```

# Matching with propensity distance

The Euclidean distance in the original attributes is just one possible choice. A popular one is to use distance in propensity score $e(\mathbf{X})=E(Z|\mathbf(X))$. To estimate the conditional expectation we used Python and provide a Jupyter Lab Notebook as the companion file Sensitivity_Analysis-toy_example.ipynb (titled Propensity score estimation and Sensitivity Analysis for bias in Nursing Homes).  

We just load the propensity score, define a distance matrix based on it and compute the corresponding inverse weights. 
```{r propensityLoad}
# Load propensity computed with python 

propensity <- read.table("Data/propensity_period_1_toy.csv",
                                  quote="\"", comment.char="")

# Computation of the distance matrix based on propensity scores

dist.propensity <- as.matrix(dist(propensity))

# Inverse Propensity Weights
weight.g.propensity <- dataNursingHomes$nursing_home / propensity +
  (1 - dataNursingHomes$nursing_home) / (1 - propensity)

# Propensity Weights
weight.ATO <- (1 - propensity) * dataNursingHomes$nursing_home + propensity * (1 - dataNursingHomes$nursing_home)
```
After that, as for the Euclidean case, we compute the matched samples. 
```{r propensityMatching}

MatchIt.propensity.nearest <- MatchIt::matchit(formula_obj,
                                               data = dataNursingHomes,
                                               method = "nearest",
                                               distance = dist.propensity)
```
# Average Treatment Effect (ATE) estimates
With the previous weighted subsamples and some alternative estimates obtained in Propensity score estimation and Sensitivity Analysis for bias in Nursing Homes one obtains the following ATEs.
```{r ateComputation}
ate.table <- array(dim = c(7,2))
ate.python <- read.csv("Data/ate_python.csv", header = FALSE, sep = ",")

# Naif approximation (as if data come from a randomized study)
ate.table[1, ] <- as.numeric(weightedMeansDifference(subset(dataNursingHomes, nursing_home == 0)$hospitalisation,
                      subset(dataNursingHomes, nursing_home == 1)$hospitalisation,
                      rep(1, sum(dataNursingHomes$nursing_home == 0)),
                      rep(1, sum(dataNursingHomes$nursing_home == 1))))

# Estimates assuming observational data

# Estimation from adjustment formula 
ate.table[2, ] <- as.numeric(ate.python[1, ])

# Estimation using matching with squared Euclidean distance
ate.table[3,] <- as.numeric(weightedMeansDifference(dataNursingHomes$hospitalisation[as.numeric(MatchIt.euc.nearest$match.matrix)],
                      subset(dataNursingHomes, nursing_home == 1)$hospitalisation,
                      rep(1, length(as.numeric(MatchIt.euc.nearest$match.matrix))),
                      rep(1, sum(dataNursingHomes$nursing_home == 1))))

# Estimation using matching with squared Propensity distance
ate.table[4, ] <- as.numeric(weightedMeansDifference(dataNursingHomes$hospitalisation[as.numeric(MatchIt.propensity.nearest$match.matrix)],
                      subset(dataNursingHomes, nursing_home == 1)$hospitalisation,
                      rep(1, length(as.numeric(MatchIt.propensity.nearest$match.matrix))),
                      rep(1, sum(dataNursingHomes$nursing_home == 1))))

# Estimation using a doubly robust estimator
ate.table[5, ] <- as.numeric(ate.python[2, ])

# The classical classical estimator based on inverse propensity weighting
ate.table[6, ] <- as.numeric(ate.python[3, ])

# An estimator based on propensity weighting, usually known as Average Treatment on the Overlap (ATO) 
ate.table[7, ] <- as.numeric(weightedMeansDifference(subset(dataNursingHomes, nursing_home == 0)$hospitalisation,
                      subset(dataNursingHomes, nursing_home == 1)$hospitalisation,
                      weight.ATO$V1[which(dataNursingHomes$nursing_home == 0)],
                      weight.ATO$V1[which(dataNursingHomes$nursing_home == 1)]))
colnames(ate.table) <- c("ATE", "Plus-minus")
rownames(ate.table) <- c("Naif", "Adjustment Formula", "Matched Euc", "Matched Propensity",
                                     "Inverse Prop Weighting Doubly Robust", "Inverse Prop Weighting", "Propensity Weighted")
ate.table
```
# ATE estimates quality diagnostics
To check the good behaviour of the ATE estimates used in the previous section we need to rely on some diagnostic procedures.

## Porpensity score distributions
A first one is to visually compare the distributions of the propensity scores estimates for each class, nursing home residents (Z=1) and non-residents (Z=0), for all the esimates.
```{r plottingPropensity}
plot(density(propensity$V1[which(dataNursingHomes$nursing_home == 0)]),
     main = "Comparison of propensity distributions",
     xlab = "Propensity", ylim = c(0, 6), col = "#F8766D", lwd = 8,
     cex.main = 1.4, cex.axis = 1, cex.lab = 1.2)
lines(density(propensity$V1[which(dataNursingHomes$nursing_home == 1)]), lty = 2, col = "#F8766D", lwd = 8)
lines(density(propensity$V1[as.numeric(MatchIt.propensity.nearest$match.matrix)]),
      col = "#00BA38", lwd = 7)
lines(density(propensity$V1[which(dataNursingHomes$nursing_home == 1)]),
      col = "#00BA38", lty = 2, lwd = 7)
lines(density(propensity$V1[as.numeric(MatchIt.euc.nearest$match.matrix)]),
      col = "#B79F00", lwd = 7)
lines(density(propensity$V1[which(dataNursingHomes$nursing_home == 1)]),
      col = "#B79F00", lty = 2, lwd = 7)
lines(density(propensity$V1[dataNursingHomes$nursing_home == 0],
              weights =  weight.ATO$V1[dataNursingHomes$nursing_home == 0] / 
                sum(weight.ATO$V1[dataNursingHomes$nursing_home == 0])),
      col = "#619CFF", lwd = 7)
lines(density(propensity$V1[dataNursingHomes$nursing_home == 1],
              weights = weight.ATO$V1[dataNursingHomes$nursing_home == 1] /
                sum(weight.ATO$V1[dataNursingHomes$nursing_home == 1])),
      col = "#619CFF", lty = 2, lwd = 7)
lines(density(propensity$V1[dataNursingHomes$nursing_home == 0],
              weights = weight.g.propensity$V1[dataNursingHomes$nursing_home == 0] / 
                sum(weight.g.propensity$V1[dataNursingHomes$nursing_home == 0])),
      col = "#F564E3", lwd = 7)
lines(density(propensity$V1[dataNursingHomes$nursing_home == 1],
              weights = weight.g.propensity$V1[dataNursingHomes$nursing_home == 1] /
                sum(weight.g.propensity$V1[dataNursingHomes$nursing_home == 1])), 
      col = "#F564E3", lty = 2,lwd = 7)

```
```{r legendPropensity, echo=FALSE}
plot.new()
legend(x = 0, y = 1.02, legend = c("No", "Yes"), lty = c(1, 2), lwd = rep(2, 2),
       title = "Nursing home:", horiz = TRUE)
legend(x = 0, y = 0.75, legend = c("Naif", "Matched Euc", "Matched Prop",
                                     "Propensity Weighting", "Inverse Prop Weighting"),
       fill = c("#F8766D", "#B79F00", "#00BA38", "#619CFF", "#F564E3"),
       title = "Method")
```

Clearly we see that all methods are producing propensity distributions that are more similar than the original ones in red.

## Total Variation distance (TVd) on the marginals
Another possibility is to compare the marginal distributions on the original variables. We chose to this using the well known Total Varaition distance. The wrapper function, totalVariationAnalysis, estimates the 1D TVd's for discrete and continuous variables. Bellow are the examples for the corresponding ATEs. 
```{r totalVarAnalysisEuc}
vars_of_interest = names(dataNursingHomes)[1:4]
total.variation.0 <- totalVariationAnalysis(vars = vars_of_interest, data = dataNursingHomes,
                                                        strata = "nursing_home", strata.levels = c(0, 1),
                                                        vars_cont = c("age", "time_to_positive"))
total.variation.euc.1 <- totalVariationAnalysis(vars = vars_of_interest, 
                                                data = dataNursingHomes[c(as.numeric(MatchIt.euc.nearest$match.matrix),
                                                                          which(dataNursingHomes[["nursing_home"]] == 1)), ],
                                                        strata = "nursing_home", strata.levels = c(0, 1),
                                                        vars_cont = c("age", "time_to_positive"))
```

```{r totalVarAnalysisProp}
total.variation.prop.0 <- totalVariationAnalysis(vars = vars_of_interest, data = dataNursingHomes,
                                                        strata = "nursing_home", strata.levels = c(0, 1),
                                                        vars_cont = c("age", "time_to_positive"),
                                                 weights.g = weight.g.propensity$V1)
total.variation.prop.1 <- totalVariationAnalysis(vars = vars_of_interest, data =
                                                   dataNursingHomes[c(as.numeric(MatchIt.propensity.nearest$match.matrix),
                                                                      which(dataNursingHomes[["nursing_home"]] == 1)), ],
                                                        strata = "nursing_home", strata.levels = c(0, 1),
                                                        vars_cont = c("age", "time_to_positive"))
total.variation.prop.2 <- totalVariationAnalysis(vars = vars_of_interest, data = dataNursingHomes,
                                                        strata = "nursing_home", strata.levels = c(0, 1),
                                                        vars_cont = c("age", "time_to_positive"),
                                                 weights.g = weight.ATO$V1)
```

```{r plotingTVd}
dataPlot <- data.frame(variable   =  vars_of_interest,
                       Naif  = total.variation.0,
                       "Matched Prop"   = total.variation.prop.1,
                       "Matched Euc"   = total.variation.euc.1,
                       "Inverse Prop Weighting" = total.variation.prop.0,
                       "Propensity Weighting" = total.variation.prop.2
)
dataPlot_Melt <- melt(data          = dataPlot,
                      id.vars       = c("variable"),
                      variable.name = "Method",
                      value.name    = "Total variation distance")
varNames <- as.character(dataPlot$variable)[order(dataPlot$Naif)]
dataPlot_Melt$variable <- factor(dataPlot_Melt$variable,
                                 levels = varNames)

ggplot(data = dataPlot_Melt,
       mapping = aes(x = variable, y = `Total variation distance`, group = Method, color = Method)) +
  geom_line(size = 2.5) +
  geom_point(size = 3.5) +
  coord_flip(ylim = c(0, 0.55)) +
  theme_bw() + 
  ggtitle ("Marginal TVd comparison") +
  theme(plot.title = element_text(color="black", size=24, face="bold.italic"),
        legend.key = element_blank(),
        axis.text=element_text(size=20),
        axis.title=element_text(size=22),
        legend.text = element_text(size=14))
```

One can see that reductions are significant for all but time_to_positive, for which mixed results are obtained. Hence, indeed, the 1D marginals for residents and non-residents are closer together than for the original data, especially for the estiamtors based on Propensity and Inverse Propensity Weighting.

## Multivariate similarity
It is well known that similarity in marginals does not guarantee similarity in the multivariate space.  Here we use an approache based on the $d-$Wasserstein distance, which allows weighted data. Details are provided in @biasNursing.

We start with a particular normalisation of the data.
```{r normalisedData}
dataNursingHomesNorm <- dataNursingHomes[, 1:5]
dataNursingHomesNorm[, 1] <- (dataNursingHomesNorm[, 1] - 60) / (105 - 60)
dataNursingHomesNorm[, 2] <- dataNursingHomesNorm[, 1] / 20
dataNursingHomesNorm[, 3] <- dataNursingHomesNorm[, 1] / 121
```

Then we compute the 2-Wasserstein distance for the weighed (sub)samples.
```{r multivariateDistanceWasserstein}
period.1.stats.wasser <- array(dim = c(5, 1))
dist.matrix <- as.matrix(dist(dataNursingHomesNorm[,-dim(dataNursingHomesNorm)[2]]))

# 2-Wasserstein distance for the original data corresponding to the Naif estimator
n.A <- sum(dataNursingHomesNorm$nursing_home == 0)
n.B <- sum(dataNursingHomesNorm$nursing_home == 1)
A.1 <- rep(1/n.A, n.A)
B.1 <- rep(1/n.B, n.B)

period.1.stats.wasser[1, ] <- wasserDistance(A.1, B.1, 
                                           (dist.matrix[which(dataNursingHomesNorm$nursing_home == 0),
                                                          which(dataNursingHomesNorm$nursing_home == 1)])^2,
                                           2)
# 2-Wasserstein distance for the data matched using the Euclidean distance  
n.A <- length(as.numeric(MatchIt.euc.nearest$match.matrix))
A.1 <- rep(1/n.A, n.A)
period.1.stats.wasser[2, ] <- wasserDistance(A.1, B.1,
                                           (dist.matrix[as.numeric(MatchIt.euc.nearest$match.matrix),
                                                        which(dataNursingHomes[["nursing_home"]] == 1)])^2,
                                           2)
# 2-Wasserstein distance for the data matched using the Propensity distance
n.A <- length(as.numeric(MatchIt.propensity.nearest$match.matrix))
A.1 <- rep(1/n.A, n.A)
period.1.stats.wasser[3, ] <- wasserDistance(A.1, B.1,
                                           (dist.matrix[as.numeric(MatchIt.propensity.nearest$match.matrix),
                                                        which(dataNursingHomes[["nursing_home"]] == 1)])^2,
                                           2)
# 2-Wasserstein distance for the data weighted using Inverse Propensity Weighting
A.1 <- weight.g.propensity[which(dataNursingHomesNorm$nursing_home == 0), 1]
A.1 <- A.1 / sum(A.1)
B.1 <- weight.g.propensity[which(dataNursingHomesNorm$nursing_home == 1), 1]
B.1 <- B.1 / sum(B.1)
period.1.stats.wasser[4, ] <- wasserDistance(A.1, B.1, 
                                           (dist.matrix[which(dataNursingHomesNorm$nursing_home == 0),
                                                          which(dataNursingHomesNorm$nursing_home == 1)])^2,
                                           2)
# 2-Wasserstein distance for the data weighted using Propensity Weighting
A.1 <- weight.ATO[which(dataNursingHomesNorm$nursing_home == 0), 1]
A.1 <- A.1 / sum(A.1)
B.1 <- weight.ATO[which(dataNursingHomesNorm$nursing_home == 1), 1]
B.1 <- B.1 / sum(B.1)
period.1.stats.wasser[5, ] <- wasserDistance(A.1, B.1, 
                                           (dist.matrix[which(dataNursingHomesNorm$nursing_home == 0),
                                                          which(dataNursingHomesNorm$nursing_home == 1)])^2,
                                           2)
rownames(period.1.stats.wasser) <- c("Naif", "Matched Euc", "Matched Prop", "Inverse Prop Weighting", "Propensity Weighting")
colnames(period.1.stats.wasser) <- c("2-Wasserstein")
period.1.stats.wasser
```
In all measures, we see a clear reduction in the distance in multivariate space between the residents and non-resident's joint distributions. Hence, we do have more similar samples after applying the previously described methods. It is important to notice that in our example although Inverse Propensity Weighting produces 1-d marginals that are more similar, Propensity Weighting produces joint distributions that are more similar.

# Conclusions

In this fictitious example, the ATE estimates show that there is strong discrimination against nursing home residents, since they are much lees likely to be hospitalised than their non-residents counterparts. However, one must be cautious because of residual confounding due to joint distributions not being exactly the same and due to unobserved confounding (also known as omitted variable bias). The latter is quantified with the methods presented in @sensAnalysis in Propensity score estimation and Sensitivity Analysis for bias in Nursing Homes.   

# References
